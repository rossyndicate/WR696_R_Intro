# Error + Statistical Power

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(lterdatasampler)
library(palmerpenguins)
library(rstatix)
```

The results of any statistical test run the risk of giving you the wrong answer; particularly if your data assumptions aren't being met and/or you do not have enough observations. However, these statistical misinterpretations ultimately come down to just two types of error: type I and type II error.

::: {.alert .alert-info}
The **null hypotheses** are for each test we have learned thus far:

**Levene test** - Variances across groups are equal (homogeneity of variances).

**Shapiro-Wilk test** - Data is drawn from a normal distribution.

**t-test** - Means of the two groups are equal.

**ANOVA** - Means of all groups are equal.

**Pearson correlation** - There is no linear correlation between the two variables.

**Spearman correlation** - There is no monotonic correlation between the two variables.

**Simple linear regression** - There is no linear relationship between the predictor and response variables.

**Multiple linear regression**

-   **Overall model** - All regression coefficients equal zero (i.e., there is no effect from predictors).

-   **Individual predictors** - An individual regression coefficient equals zero (no effect for a specific predictor).
:::

### **Type I error (false positive)**

Type I error occurs when we mistakenly reject a true null hypothesis. This error is often associated with setting the significance level (alpha, α) too low in hypothesis testing.

\*\*Alpha (α)\*\*, or the significance level, is a predetermined threshold used to evaluate the strength of evidence \*against\* the null hypothesis. Commonly set at 0.05, alpha defines the maximum acceptable probability of making a type I error

On the other hand, **p-values** measure the strength of the evidence against the null hypothesis based on the sample data. A p-value indicates the probability of obtaining the observed results, or more extreme results, if the null hypothesis were true. **When comparing the p-value to alpha, if the p-value is less than or equal to alpha, it suggests that the observed data provides enough evidence to reject the null hypothesis.**

In this sense, α acts as a threshold for making decisions in hypothesis testing. If p ≤ α, we reject the null hypothesis; otherwise, we fail to reject it. Thus, α and p-values are interrelated in hypothesis testing, where the former sets the standard for the strength of evidence required to make a decision about the null hypothesis based on the latter.

### **Type II error (false negative)**

Type II error occurs when we fail to reject a false null hypothesis. In other words, we fail to detect a real effect or difference when it actually exists. Type II error is associated with power, which is also known as beta.

Power depends on several factors:

1.  Sample size - Larger samples provide more power.

2.  Effect size - Larger or more dramatic effects are easier to detect.

3.  Significance level (α/subsequent p-value threshold) - Higher significance levels require more power.

### **Power**

Power analysis is used to calculate the minimum sample size required to have a high chance of detecting an effect of a given magnitude, and at a desired significance level.

Let's perform a t-test analysis on our Palmer penguins data set, where we compare bill length across two penguins species, Gentoo and Adelie.

```{r}
data("penguins")

penguins %>%
  filter(species %in% c("Adelie", "Gentoo")) %>%
  # species must be re-formatted as text to get rid of previous factoring.
  # this needs to happen for t_test to work later.
  mutate(species = as.character(species)) %>% 
  t_test(bill_length_mm ~ species, var.equal = FALSE, detailed = TRUE)
```

#### Sample size

In our analysis above, we find that bill length is observed to be significantly higher (at an alpha of 0.05) in Gentoo penguins when compared to Adelie penguins. But, what if we didn't have as many observations to perform our test on? Let's cut our data set down to just two observations per species:

```{r}
penguins %>%
  filter(species %in% c("Adelie", "Gentoo")) %>%
  # species must be re-formatted as text to get rid of previous factoring.
  # this needs to happen for t_test to work later.
  mutate(species = as.character(species)) %>% 
  drop_na(bill_length_mm) %>%
  group_by(species) %>%
  slice_sample(n = 2) %>%
  ungroup() %>%
  t_test(bill_length_mm ~ species, var.equal = FALSE, detailed = TRUE)
```

With a sample of just n=2 penguins of each species, we do not have enough power to detect a significant difference in bill length between species. Comparing our p-value above to an α of 0.05, it is clear that we fail to reject the null hypothesis. Therefore, as sample size decreases, so does our power in identifying true trends.

#### Magnitude of effect

Let's look at a histogram of our bill lengths across our two penguin species:

```{r}
ggplot(data = penguins %>% 
         filter(species %in% c("Adelie", "Gentoo")) %>%
         # species must be re-formatted as text to get rid of previous factoring.
         # this needs to happen for t_test to work later.
         mutate(species = as.character(species))) +
  geom_histogram(aes(x = bill_length_mm, fill = species), alpha = 0.56) +
  geom_vline(xintercept =  38.8) +
  geom_vline(xintercept = 47.5)
```

In the grand scheme of things the difference between these two populations is relatively small in magnitude: their histograms overlap, indicating that some penguins in both species often have similar bill lengths. But, what if all penguins in our Gentoo species magically grew an extra 15 mm's of bill length?

```{r}
ggplot(data = penguins %>% 
         filter(species %in% c("Adelie", "Gentoo")) %>%
         mutate(bill_length_mm = ifelse(species == "Gentoo", bill_length_mm + 15, bill_length_mm)) %>%
         # species must be re-formatted as text to get rid of previous factoring.
         # this needs to happen for t_test to work later.
         mutate(species = as.character(species))) +
  geom_histogram(aes(x = bill_length_mm, fill = species), alpha = 0.56) +
  geom_vline(xintercept =  38.8) +
  geom_vline(xintercept = 62.5)

penguins %>%
  filter(species %in% c("Adelie", "Gentoo")) %>%
  # species must be re-formatted as text to get rid of previous factoring.
  # this needs to happen for t_test to work later.
  mutate(species = as.character(species)) %>%
  drop_na(bill_length_mm) %>%
                          #if species == Gentoo, add 15 to bill length..
  mutate(bill_length_mm = ifelse(species == "Gentoo", bill_length_mm + 15, 
                                 # otherwise, keep it as is.
                                 bill_length_mm)) %>%
    t_test(bill_length_mm ~ species, var.equal = FALSE, detailed = TRUE)
```

... when the magnitude of the difference between our observations in our groups increases, our p-value decreases. And, when you have a greater difference between populations, the number of observations required to identify significant differences generally does not have to be so high:

```{r}
penguins %>%
  filter(species %in% c("Adelie", "Gentoo")) %>%
  # species must be re-formatted as text to get rid of previous factoring.
  # this needs to happen for t_test to work later.
  mutate(species = as.character(species)) %>% 
  mutate(bill_length_mm = ifelse(species == "Gentoo", bill_length_mm + 15, bill_length_mm)) %>%
  # remove NA values for weight
  drop_na(bill_length_mm) %>%
  group_by(species) %>%
  # select 5 random observations from each forest type:
  slice_sample(n = 2) %>%
  ungroup() %>%
  t_test(bill_length_mm ~ species, var.equal = FALSE, detailed = TRUE)
```

### Note about random selection

If you are playing around with this code on your own, you may notice that the results of each test look different than what's written in the bookdown: this is because we are randomly selecting a subset from our penguins populations. In fact, you may have encountered p-values that lead to different conclusions for you (especially those with low sample sizes). The way we select samples from a population significantly impacts our statistical results and the statistical power of our tests. If our samples are representative of the population and sufficiently large, our findings are more likely to accurately reflect reality. However, if our samples are small or not truly representative, our results may be less reliable, and our tests may lack the ability to detect real effects.

## Assignment

Let's re-explore the difference in weight of cutthroat trout in clear cut (CC) and old growth (OG) forest types (Chapter 5). We want to see how sample size affects our ability to detect this difference. Therefore, our research question is: "Is there a significant difference in weight between old growth and clear cut forest types?" **We will set our significance level, alpha, at 0.05.**

First load in your data:

```{r}
data(and_vertebrates)
and_vertebrates %>% 
  #filter species (remember spelling and capitalization are IMPORTANT)
  filter(species == "Cutthroat trout") %>% 
  # remove NA values for weight
  drop_na(weight_g) 
```

Next, we will select a random set of trout observations at both forest types across four different sample sizes: 5, 10, 1000, 5000. Here, I have created the first object of 5 observations per forest type for you:

```{r, eval = FALSE, echo = FALSE}

c5 <- and_vertebrates %>% 
  #filter species (remember spelling and capitalization are IMPORTANT)
  filter(species == "Cutthroat trout") %>% 
  # remove NA values for weight
  drop_na(weight_g) %>%
  group_by(section) %>%
  slice_sample(n = 5) %>%
  ungroup() %>%
  t_test(weight_g ~ section, var.equal = FALSE, detailed = TRUE)
```

**1.** Write a function called `trout_subber` that filters our `and_vertebrates` to cutthroat trout only, and takes a user-selected number of random observations across both forest types from that data set. HINTS: the number of observations will be an argument of the function. The code I've written above can be used as the basis of the function. Follow steps in the Posit Primer, [How to write a function - Workflow](https://posit.cloud/learn/primers/6.2). - .

```{r, echo = FALSE, eval = FALSE}

trout_subber <- function(df, no){
  
  df %>%
  #filter species (remember spelling and capitalization are IMPORTANT)
  filter(species == "Cutthroat trout") %>% 
  # remove NA values for weight
  drop_na(weight_g) %>%
  group_by(section) %>%
  slice_sample(n = no) %>%
  ungroup() 

  
}

# test it:
p150 <- trout_subber(df = and_vertebrates, no = 150)

```

**2.** Build upon the previous function by adding an additional step to perform a t-test on the data set, and to return the results of that t-test. (NOTE: for simplicity, use the non-parametric t-test across all sub sets).

```{r, eval = FALSE, echo = FALSE}

trout_subber <- function(df, no){
  
    df %>%
  #filter species (remember spelling and capitalization are IMPORTANT)
  filter(species == "Cutthroat trout") %>% 
  # remove NA values for weight
  drop_na(weight_g) %>%
  group_by(section) %>%
  slice_sample(n = no) %>%
  ungroup()  %>%
  t_test(weight_g ~ section, var.equal = FALSE, detailed = TRUE)
  
}

t10 <- trout_subber(and_vertebrates, no = 10)
```

**3.** Map over the function above, using our sample sizes of interest (i.e., 5, 10, 1000, 5000 per forest type). Repeat the process 100 times for each sample size to account for variability. The final outcome of this exercise should be a single data frame with rows that includes all of our t-test summaries stacked on top of each other. HINT: what does \`rep()\` do?

```{r, eval = FALSE, echo = FALSE}
final_data <- rep(c(5, 10, 1000, 5000), 100)  %>%
  map(~trout_subber(df = and_vertebrates, no = .x)) %>%
  bind_rows()
```

**4.** Using the data frame created in exercise 3, develop a boxplot of p-values grouped by sample size. How do our p-values change with sample size?

```{r, eval = FALSE, echo = FALSE}
ggplot(final_data) +
  geom_boxplot(aes(y = p)) +
  facet_wrap(~n1, scales = "free_y")
```

## Citations

***Data Source:*** Gregory, S.V. and I. Arismendi. 2020. Aquatic Vertebrate Population Study in Mack Creek, Andrews Experimental Forest, 1987 to present ver 14. Environmental Data Initiative. <https://doi.org/10.6073/pasta/7c78d662e847cdbe33584add8f809165>

Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. <https://allisonhorst.github.io/palmerpenguins/>. doi: 10.5281/zenodo.3960218.
